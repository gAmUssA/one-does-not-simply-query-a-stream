= RisingWave with Confluent Cloud Integration

This example demonstrates how to use RisingWave for real-time stream processing with Confluent Cloud Kafka and Schema Registry.

== Prerequisites

1. **Confluent Cloud Account**: Set up a Kafka cluster and Schema Registry in Confluent Cloud
2. **API Keys**: Create API keys for both Kafka and Schema Registry
3. **Environment Configuration**: Copy `.env.example` to `.env` and configure your Confluent Cloud settings

== Step 1: Configure Environment

Copy the example environment file and configure your Confluent Cloud credentials:

[source,sh]
----
cp ../.env.example ../.env
----

Edit the `.env` file with your Confluent Cloud settings:

[source,properties]
----
BOOTSTRAP_SERVERS=your-cluster.region.provider.confluent.cloud:9092
SCHEMA_REGISTRY_URL=https://your-schema-registry.region.provider.confluent.cloud
SCHEMA_REGISTRY_API_KEY=your-schema-registry-api-key
SCHEMA_REGISTRY_API_SECRET=your-schema-registry-api-secret
SASL_JAAS_CONFIG="org.apache.kafka.common.security.plain.PlainLoginModule required username='your-kafka-api-key' password='your-kafka-api-secret';"
TOPIC_NAME=flights
----

== Step 2: Start RisingWave

Start RisingWave with Confluent Cloud integration:

[source,sh]
----
make start
----

This will:
- Check your `.env` configuration
- Start RisingWave connected to Confluent Cloud
- Wait for RisingWave to be ready

== Step 3: Connect to RisingWave

Connect to the RisingWave database using the psql client:

[source,sh]
----
make run-client
----

Or manually:

[source,sh]
----
docker compose exec -it psql-client psql -h risingwave -p 4566 -d dev -U root
----

== Step 4: Create Sources and Process Data

=== Option A: Use the Flights Example

Load the comprehensive flights processing example:

[source,sh]
----
# Copy the SQL content from flights-example.sql and paste into the RisingWave client
----

This creates:
- A source reading from the `flights` topic in Confluent Cloud
- Real-time views for delayed flights, airline performance, and airport congestion
- A sink sending alerts back to Confluent Cloud

=== Option B: Use General Examples

Load the general Confluent Cloud examples:

[source,sh]
----
# Copy the SQL content from confluent-cloud-examples.sql and paste into the RisingWave client
----

This includes examples for:
- JSON and Avro sources with Schema Registry
- Materialized views for stream processing
- Sinks back to Confluent Cloud
- Temporal queries and windowing

== Step 5: Monitor and Query

=== Check System Status

[source,sql]
----
-- View all sources
SELECT * FROM rw_sources;

-- View all materialized views  
SELECT * FROM rw_materialized_views;

-- View all sinks
SELECT * FROM rw_sinks;
----

=== Query Real-time Data

[source,sql]
----
-- View recent flight data
SELECT * FROM flights_stream ORDER BY scheduledDeparture DESC LIMIT 10;

-- Monitor delayed flights
SELECT * FROM delayed_flights_live ORDER BY delay_minutes DESC LIMIT 20;

-- Check airline performance
SELECT * FROM airline_performance ORDER BY delay_percentage DESC;
----

== Available Make Targets

[source,sh]
----
make help          # Show all available targets
make start         # Start RisingWave with Confluent Cloud
make status        # Check RisingWave status
make info          # Show connection information
make run-client    # Connect to RisingWave with psql
make logs          # Show RisingWave logs
make destroy       # Stop and cleanup containers
----

== Architecture Overview

1. **Confluent Cloud Kafka**: Source of streaming data (flights, events, etc.)
2. **Confluent Cloud Schema Registry**: Manages Avro schemas for structured data
3. **RisingWave**: Stream processing engine with SQL interface
4. **Materialized Views**: Real-time aggregations and transformations
5. **Sinks**: Send processed results back to Confluent Cloud topics

== Key Features Demonstrated

=== Real-time Stream Processing
- Continuous processing of Kafka streams
- SQL-based transformations and aggregations
- Materialized views for instant query results

=== Confluent Cloud Integration
- Secure SASL_SSL connections to Confluent Cloud
- Schema Registry integration for Avro data
- Environment-based configuration management

=== Advanced Analytics
- Temporal windowing (tumbling, hopping windows)
- Complex event processing
- Multi-stream joins and correlations

=== Operational Monitoring
- Real-time dashboards via materialized views
- Alert generation and routing
- Performance metrics and KPIs

== Troubleshooting

=== Connection Issues

[source,sh]
----
# Check environment configuration
make check-env

# View RisingWave logs
make logs

# Check container status
docker compose ps
----

=== Common Issues

1. **Authentication Errors**: Verify API keys and SASL configuration in `.env`
2. **Schema Registry Issues**: Ensure Schema Registry URL and credentials are correct
3. **Topic Not Found**: Verify topic exists in Confluent Cloud and permissions are set
4. **Network Issues**: Check firewall settings and Confluent Cloud IP allowlists

=== Debugging SQL

[source,sql]
----
-- Check source status
DESCRIBE flights_stream;

-- View source data sample
SELECT * FROM flights_stream LIMIT 5;

-- Check materialized view refresh
SELECT * FROM rw_materialized_views WHERE name = 'delayed_flights_live';
----

== Next Steps

1. **Scale Processing**: Add more compute nodes for higher throughput
2. **Add More Sources**: Connect additional Kafka topics and external systems  
3. **Advanced Analytics**: Implement machine learning models and complex CEP patterns
4. **Monitoring**: Set up Grafana dashboards and alerting
5. **Production Deployment**: Configure high availability and disaster recovery

For more examples and advanced configurations, see the SQL files in this directory:
- `confluent-cloud-examples.sql` - General integration patterns
- `flights-example.sql` - Complete flights processing pipeline